{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822c2233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/21 15:41:12 WARN Utils: Your hostname, promo-ds4-gra9-6, resolves to a loopback address: 127.0.1.1; using 57.128.77.93 instead (on interface ens3)\n",
      "25/11/21 15:41:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/21 15:41:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/21 15:41:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"test\")\n",
    "l=[1,2,3,4,5]\n",
    "rdd = sc.parallelize(l)\n",
    "rdd.saveAsTextFile(\"hdfs://172.30.0.2:9000/data_test/validate_skill2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e87247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8a1c2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 15:44:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ArbresParis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69b09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"delimiter\", \";\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .csv(\"hdfs://172.30.0.2:9000/data_test/arbres.csv\")\n",
    "               #.csv(\"arbresremarquablesparis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8944d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 15:44:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/11/21 15:44:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Geo point, idbase, domanialite, arrondissement, complement adresse, numero, adresse, circonference en cm, hauteur en m, stade développement, pépinière, genre, espèce, varieteoucultivar, date de plantation, libellé Français, ID Base, ID arbre, Site, Adresse, Complément d'adresse, Arrondissement, Domanialité, Dénomination usuelle, Dénomination botanique, Autorité taxonomique, Année de plantation, Qualification remarquable, Résumé, Descriptif, Numéro de délibération, Date de la délibération, Label national, Panonceau, Photo 1, Copyright 1\n",
      " Schema: Geo point, idbase, domanialite, arrondissement3, complement adresse, numero, adresse6, circonference en cm, hauteur en m, stade développement, pépinière, genre, espèce, varieteoucultivar, date de plantation, libellé Français, ID Base, ID arbre, Site, Adresse19, Complément d'adresse, Arrondissement21, Domanialité, Dénomination usuelle, Dénomination botanique, Autorité taxonomique, Année de plantation, Qualification remarquable, Résumé, Descriptif, Numéro de délibération, Date de la délibération, Label national, Panonceau, Photo 1, Copyright 1\n",
      "Expected: arrondissement3 but found: arrondissement\n",
      "CSV file: hdfs://172.30.0.2:9000/data_test/arbres.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------+----------------+--------------------+------+--------------------+-------------------+------------+-------------------+---------+------------+-------------+-----------------+-------------------+--------------------+---------+---------+--------------------+--------------------+--------------------+----------------+----------------+--------------------+----------------------+--------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|           Geo point|   idbase|domanialite| arrondissement3|  complement adresse|numero|            adresse6|circonference en cm|hauteur en m|stade développement|pépinière|       genre|       espèce|varieteoucultivar| date de plantation|    libellé Français|  ID Base| ID arbre|                Site|           Adresse19|Complément d'adresse|Arrondissement21|     Domanialité|Dénomination usuelle|Dénomination botanique|Autorité taxonomique|Année de plantation|Qualification remarquable|              Résumé|          Descriptif|Numéro de délibération|Date de la délibération|Label national|           Panonceau|             Photo 1|         Copyright 1|\n",
      "+--------------------+---------+-----------+----------------+--------------------+------+--------------------+-------------------+------------+-------------------+---------+------------+-------------+-----------------+-------------------+--------------------+---------+---------+--------------------+--------------------+--------------------+----------------+----------------+--------------------+----------------------+--------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|48.87354080359454...| 114869.0|     Jardin| PARIS 16E ARRDT|PELOUSE 10 - 20 à 26|  NULL|JARDIN DE L AVENU...|              174.0|        10.0|                  A| Inconnue|Koelreuteria|   paniculata|             NULL|1700-01-01 00:09:21|           Savonnier| 114869.0| 114869.0|         Avenue Foch|      10 Avenue Foch|PELOUSE 10 - 20 à 26|              16|          Jardin|           Savonnier|  Koelreuteria pani...|               Laxm.|           Inconnue|                 Paysager|Cet arbre est cla...|Originaire du nor...|                  NULL|                   NULL|          NULL|https://capgeo.si...|https://capgeo.si...|Clément Dorval / ...|\n",
      "|48.86326316465342...|2002347.0|     Jardin|BOIS DE BOULOGNE|               16-07|  NULL|GRANDE CASCADE - ...|              330.0|        30.0|                  M| Inconnue|    Taxodium|    distichum|             NULL|1859-01-01 00:09:21|       Cyprès Chauve|2002347.0|2002427.0|Bois de Boulogne....|Carrefour de Long...|               16-07|              16|Bois de Boulogne|       Cyprès chauve|    Taxodium distichum|          (L.) Rich.|               1859|                 Paysager|Cet arbre est cla...|Le cyprès chauve ...|                  NULL|                   NULL|          NULL|https://capgeo.si...|https://capgeo.si...|Sonia Yassa / Vil...|\n",
      "|48.85974252900819...| 147672.0|  CIMETIERE| PARIS 20E ARRDT|               20-15|  NULL|CIMETIERE DU PERE...|              363.0|        22.0|                  M| Inconnue|    Aesculus|hippocastanum|             NULL|1700-01-01 00:09:21|          Marronnier| 147672.0| 147672.0|Cimetière du Père...|Cimetiere Du Pere...|               20-15|              20|       Cimetière|   Marronnier d'Inde|  Aesculus hippocas...|                  L.|           Inconnue|                 Paysager|Cet arbre est cla...|Le marronnier est...|                  NULL|                   NULL|          NULL|https://capgeo.si...|https://capgeo.si...|Mathieu Bedel / V...|\n",
      "|48.89335545445399...| 120016.0|     Jardin| PARIS 18E ARRDT|               18-08|  NULL|SQUARE MAURICE KR...|              234.0|        16.0|                  M| Inconnue|       Toona|     sinensis|             NULL|1700-01-01 00:09:21|             Cedrele| 120016.0| 120016.0|Square Kriegel-Va...|                NULL|               18-08|              18|          Jardin|    Cédrèle de Chine|        Toona sinensis|     (Juss.) M.Roem.|           Inconnue|                 Paysager|Cet arbre est cla...|Originaire d'Asie...|                  NULL|                   NULL|          NULL|https://capgeo.si...|https://capgeo.si...|Sonia Yassa / Vil...|\n",
      "|48.86583362319786...| 121940.0|     Jardin| PARIS 20E ARRDT|               20-06|  NULL|SQUARE EDOUARD VA...|              160.0|        16.0|                  M| Inconnue|     Corylus|      colurna|             NULL|1700-01-01 00:09:21|Noisetier de Byzance| 121940.0| 121940.0|Square Édouard Va...|  50 Avenue Gambetta|               20-06|              20|          Jardin|Noisetier de Byzance|       Corylus colurna|                  L.|           Inconnue|                 Paysager|Cet arbre est cla...|Cette essence, or...|                  NULL|                   NULL|          NULL|https://capgeo.si...|https://capgeo.si...|Mathieu Bedel / V...|\n",
      "+--------------------+---------+-----------+----------------+--------------------+------+--------------------+-------------------+------------+-------------------+---------+------------+-------------+-----------------+-------------------+--------------------+---------+---------+--------------------+--------------------+--------------------+----------------+----------------+--------------------+----------------------+--------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644f335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 15:45:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Geo point, idbase, domanialite, arrondissement, complement adresse, numero, adresse, circonference en cm, hauteur en m, stade développement, pépinière, genre, espèce, varieteoucultivar, date de plantation, libellé Français, ID Base, ID arbre, Site, Adresse, Complément d'adresse, Arrondissement, Domanialité, Dénomination usuelle, Dénomination botanique, Autorité taxonomique, Année de plantation, Qualification remarquable, Résumé, Descriptif, Numéro de délibération, Date de la délibération, Label national, Panonceau, Photo 1, Copyright 1\n",
      " Schema: Geo point, idbase, domanialite, arrondissement3, complement adresse, numero, adresse6, circonference en cm, hauteur en m, stade développement, pépinière, genre, espèce, varieteoucultivar, date de plantation, libellé Français, ID Base, ID arbre, Site, Adresse19, Complément d'adresse, Arrondissement21, Domanialité, Dénomination usuelle, Dénomination botanique, Autorité taxonomique, Année de plantation, Qualification remarquable, Résumé, Descriptif, Numéro de délibération, Date de la délibération, Label national, Panonceau, Photo 1, Copyright 1\n",
      "Expected: arrondissement3 but found: arrondissement\n",
      "CSV file: hdfs://172.30.0.2:9000/data_test/arbres.csv\n",
      "25/11/21 15:49:54 ERROR FileFormatWriter: Aborting job bcb9b61a-d4f2-437b-9e58-e6212dca9c8a.\n",
      "java.net.SocketTimeoutException: Call From promo-ds4-gra9-6/127.0.1.1 to 172.30.0.2:9000 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.30.0.1:45230 remote=172.30.0.2/172.30.0.2:9000]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:483)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:961)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:896)\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1529)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1426)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)\n",
      "\tat jdk.proxy2/jdk.proxy2.$Proxy37.getListing(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.lambda$getListing$21(ClientNamenodeProtocolTranslatorPB.java:621)\n",
      "\tat org.apache.hadoop.ipc.internal.ShadedProtobufHelper.ipc(ShadedProtobufHelper.java:160)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:621)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:437)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:170)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:162)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:100)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:366)\n",
      "\tat jdk.proxy2/jdk.proxy2.$Proxy38.getListing(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1733)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1717)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:1112)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:148)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1187)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1184)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:1194)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.30.0.1:45230 remote=172.30.0.2/172.30.0.2:9000]\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n",
      "\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n",
      "\tat java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\n",
      "\tat java.base/java.io.FilterInputStream.read(FilterInputStream.java:79)\n",
      "\tat java.base/java.io.FilterInputStream.read(FilterInputStream.java:79)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:518)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\n",
      "\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1923)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1203)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1094)\n"
     ]
    }
   ],
   "source": [
    "df.write.csv('hdfs://172.30.0.2:9000/data_test/arbres_finaux.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc18581",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43msc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m.option(\u001b[33m\"\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      2\u001b[39m                .option(\u001b[33m\"\u001b[39m\u001b[33mdelimiter\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      3\u001b[39m                .option(\u001b[33m\"\u001b[39m\u001b[33minferSchema\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      4\u001b[39m                .csv(\u001b[33m\"\u001b[39m\u001b[33mhdfs://172.30.0.2:9000/data_test/arbres.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SparkContext' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "df = sc.read.option(\"header\", \"true\") \\\n",
    "               .option(\"delimiter\", \";\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .csv(\"hdfs://172.30.0.2:9000/data_test/arbres.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
